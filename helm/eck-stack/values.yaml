---
eck-elasticsearch:
  volumeClaimDeletePolicy: DeleteOnScaledownAndClusterDeletion
  nodeSets:
  - name: default
    count: 5
    volumeClaimTemplates:
    - metadata:
        name: elasticsearch-data # Do not change this name unless you set up a corresponding persistentVolumeClaim to fulfill the request.
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 10Gi
        storageClassName: managed-csi
    config:
      # Comment out when setting the vm.max_map_count via initContainer, as these are mutually exclusive.
      # For production workloads, it is strongly recommended to increase the kernel setting vm.max_map_count to 262144
      # and leave node.store.allow_mmap unset.
      # ref: https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-virtual-memory.html
      #
      #node.store.allow_mmap: false
      search.max_async_search_response size 20965760
    podTemplate:
      spec:
        #This init container ensures that max_map_count setting is applied before starting Elasticsearch
        initContainers:
        - name: max-map-count-check
          command: ['sh', '-c', "while true; do mmc=$(cat /proc/sys/vm/max_map_count); if [ ${mmc} -eq 262144 ]; then exit 0; fi; sleep 1; done"]
        containers:
        - name: elasticsearch
          resources:
            limits:
              memory: 4Gi
              cpu: 4
            requests:
              memory: 8Gi
              cpu: 8
          env:
          - name: ES_JAVA_OPTS
            value: -Xms4096m -Xmx8192m
    service:
      type: ClusterIP
eck-kibana:
  enabled: true
  spec:
    count: 1
    elasticsearchRef:
      name: elasticsearch
    podTemplate:
      spec:
        containers:
        - name: kibana
          env:
            - name: NODE_OPTIONS
              value: "--max-old-space-size=2048"
          resources:
            requests:
              memory: 2Gi
              cpu: 2
            limits:
              memory: 5Gi
              cpu: 4
    http:
      service:
        spec:
          type: ClusterIP
eck-beats:
  enabled: true
  spec:
    type: filebeat
    daemonSet: null
    config:
      filebeat.inputs:
      - type: log
        paths:
          - /data/logstash-tutorial.log
      processors:
      - add_host_metadata: {}
      - add_cloud_metadata: {}
      output.logstash:
        # This needs to be {{logstash-name}}-ls-beats:5044
        hosts: ["logstash-ls-beats-ls-beats:5044"]
    deployment:
      podTemplate:
        spec:
          automountServiceAccountToken: true
          initContainers:
            - name: download-tutorial
              image: curlimages/curl
              command: ["/bin/sh"]
              args: ["-c", "curl -L https://download.elastic.co/demos/logstash/gettingstarted/logstash-tutorial.log.gz | gunzip -c > /data/logstash-tutorial.log"]
              volumeMounts:
                - name: data
                  mountPath: /data
          containers:
            - name: filebeat
              resources:
                requests:
                  memory: 4Gi
                  cpu: 2
                limits:
                  memory: 8Gi
                  cpu: 4
              securityContext:
                runAsUser: 1000
              volumeMounts:
                - name: data
                  mountPath: /data
                - name: beat-data
                  mountPath: /usr/share/filebeat/data
          volumes:
            - name: data
              emptydir: {}
            - name: beat-data
              emptydir: {}
eck-logstash:
  enabled: true
  # This is required to be able to set the logstash
  # output of beats in a consistent manner.
  fullnameOverride: "logstash-ls-beats"
  elasticsearchRefs:
    # This clusterName is required to match the environment variables
    # used in the below config.string output section.
    - clusterName: eck
      name: elasticsearch
  volumeClaimTemplates:
  - metadata:
      name: elasticsearch-data # Do not change this name unless you set up a corresponding persistentVolumeClaim to fulfill the request.
    spec:
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 10Gi
      storageClassName: managed-csi
  config:
    #nothing for now
  podTemplate:
    spec:
      containers:
      - name: logstash
        resources:
          requests:
            memory: 4Gi
            cpu: 2
          limits:
            memory: 8Gi
            cpu: 4
        env:
        - name: LS_JAVA_OPTS
          value: -Xms4096m -Xmx8192m
  pipelines:
    - pipeline.id: http
      config.string: |
        # HTTP/S event collector
        input {
          http {
            id => "http"
            host => "0.0.0.0"
            port => "7080"
            type => hec
            ssl_enabled => false
          }
        }  
        output {
          elasticsearch {
            index => "logstash-hec-%{+YYYY.MM.dd}"
            hosts => [ "${ECK_ES_HOSTS}" ]
            user => "${ECK_ES_USER}"
            password => "${ECK_ES_PASSWORD}"
            ssl_verification_mode => none
            #ssl_certificate_authorities => "${ECK_ES_SSL_CERTIFICATE_AUTHORITY}"
          }
        }
    - pipeline.id: syslog
      config.string: |
        # TCP and UDP Syslog inputs
        input {
          tcp {
            port => 7514
            type => syslog
            codec => plain { charset=>"ASCII" }
          }
          udp {
            port => 7514
            type => syslog
            codec => plain { charset=>"ASCII" }
          }
        }
        filter {
          if [type] == "syslog" {
            grok {
              match => { "message" => "%{SYSLOGTIMESTAMP:syslog_timestamp} %{SYSLOGHOST:syslog_hostname} %{DATA:syslog_program}(?:\[%{POSINT:syslog_pid}\])?: %{GREEDYDATA:syslog_message}" }
              add_field => [ "received_at", "%{@timestamp}" ]
              add_field => [ "received_from", "%{host}" ]
            }
            date {
              match => [ "syslog_timestamp", "MMM  d HH:mm:ss", "MMM dd HH:mm:ss" ]
            }
          }
        }
        output {
          stdout { codec => plain }
          elasticsearch {
            index => "logstash-syslog-%{+YYYY.MM.dd}"
            hosts => [ "${ECK_ES_HOSTS}" ]
            user => "${ECK_ES_USER}"
            password => "${ECK_ES_PASSWORD}"
            ssl_verification_mode => none
            #ssl_certificate_authorities => "${ECK_ES_SSL_CERTIFICATE_AUTHORITY}"
          }
        }
    - pipeline.id: main
      config.string: |
        #filebeat input
        input {
          beats {
            port => 5044
          }
        }
        filter {
          grok {
            match => { "message" => "%{HTTPD_COMMONLOG}"}
          }
          geoip {
            source => "[source][address]"
            target => "[source]"
          }
        }
        output {
          elasticsearch {
            hosts => [ "${ECK_ES_HOSTS}" ]
            user => "${ECK_ES_USER}"
            password => "${ECK_ES_PASSWORD}"
            ssl_verification_mode => none
            #ssl_certificate_authorities => "${ECK_ES_SSL_CERTIFICATE_AUTHORITY}"
          }
        }
  services:
    - name: beats
      service:
        spec:
          type: ClusterIP
          ports:
            - port: 5044
              name: "filebeat"
              protocol: TCP
              targetPort: 5044
    - name: syslog-tcp
      service:
        spec:
          type: ClusterIP
          ports:
            - port: 7514
              name: "syslog-tcp"
              protocol: TCP
              targetPort: 7514
    - name: syslog-udp
      service:
        spec:
          type: ClusterIP
          ports:
            - port: 7514
              name: "syslog-udp"
              protocol: UDP
              targetPort: 7514
    - name: hec
      service:
        spec:
          type: ClusterIP
          ports:
            - port: 7080
              name: "http-event-collector"
              protocol: TCP
              targetPort: 7080