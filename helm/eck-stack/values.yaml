---
eck-elasticsearch:
  volumeClaimDeletePolicy: DeleteOnScaledownAndClusterDeletion
  nodeSets:
  - name: masters
    count: 5
    volumeClaimTemplates:
    - metadata:
        name: elasticsearch-data # Do not change this name unless you set up a corresponding persistentVolumeClaim to fulfill the request.
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 100Gi
        storageClassName: managed-csi #this must match the persistent volume storage provider driver-name
    config:
      # Comment out when setting the vm.max_map_count via initContainer, as these are mutually exclusive.
      # For production workloads, it is strongly recommended to increase the kernel setting vm.max_map_count to 262144
      # and leave node.store.allow_mmap unset.
      # ref: https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-virtual-memory.html
      #
      #node.store.allow_mmap: false
      #search.max_async_search_response size 20965760
    podTemplate:
      spec:
        priorityClassName: elastic-cluster-high-priority
        #This init container ensures that max_map_count setting is applied before starting Elasticsearch
        initContainers:
        - name: sysctl
          securityContext:
            privileged: true
            runAsUser: 0
          command: ['sh', '-c', 'sysctl -w vm.max_map_count=262144']
        containers:
        - name: elasticsearch
          resources:
            limits:
              memory: 8Gi
              cpu: 8
            requests:
              memory: 4Gi
              cpu: 4
  - name: data
    count: 1
    config:
      node.roles: ["data", "ingest"]
    podTemplate:
      spec:
        priorityClassName: elastic-cluster-high-priority
        containers:
        - name: elasticsearch
          resources:
            limits:
              cpu: 4
              memory: 8Gi
            requests:
              cpu: 2
              memory: 4Gi
        initContainers:
        - name: sysctl
          securityContext:
            privileged: true
            runAsUser: 0
          command: ['sh', '-c', 'sysctl -w vm.max_map_count=262144']
    volumeClaimTemplates:
    - metadata:
        name: elasticsearch-data
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 100Gi
  - name: data-warm
    count: 1
    config:
      node.roles: ["data", "data_warm"]
    podTemplate:
      spec:
        priorityClassName: elastic-cluster-high-priority
        containers:
        - name: elasticsearch
          resources:
            limits:
              cpu: 4
              memory: 8Gi
            requests:
              cpu: 2
              memory: 4Gi
        initContainers:
        - name: sysctl
          securityContext:
            privileged: true
            runAsUser: 0
          command: ['sh', '-c', 'sysctl -w vm.max_map_count=262144']
    volumeClaimTemplates:
    - metadata:
        name: elasticsearch-data
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 150Gi
eck-kibana:
  enabled: true
  spec:
    count: 1
    elasticsearchRef:
      name: elasticsearch
    podTemplate:
      spec:
        containers:
        - name: kibana
          env:
            - name: NODE_OPTIONS
              value: "--max-old-space-size=4096"
          resources:
            requests:
              memory: 4Gi
              cpu: 4
            limits:
              memory: 8Gi
              cpu: 8
    http:
      service:
        spec:
          type: ClusterIP
eck-beats:
  enabled: true
  spec:
    type: filebeat
    daemonSet: null
    config:
      filebeat.inputs:
      - type: log
        paths:
          - /data/logstash-tutorial.log
      processors:
      - add_host_metadata: {}
      - add_cloud_metadata: {}
      output.logstash:
        # This needs to be {{logstash-name}}-ls-beats:5044
        hosts: ["logstash-ls-beats-ls-beats:5044"]
    deployment:
      podTemplate:
        spec:
          automountServiceAccountToken: true
          priorityClassName: filebeat-high-priority
          initContainers:
            - name: download-tutorial
              image: curlimages/curl
              command: ["/bin/sh"]
              args: ["-c", "curl -L https://download.elastic.co/demos/logstash/gettingstarted/logstash-tutorial.log.gz | gunzip -c > /data/logstash-tutorial.log"]
              volumeMounts:
                - name: data
                  mountPath: /data
          containers:
            - name: filebeat
              resources:
                requests:
                  memory: 2Gi
                  cpu: 2
                limits:
                  memory: 4Gi
                  cpu: 4
              securityContext:
                runAsUser: 1000
              volumeMounts:
                - name: data
                  mountPath: /data
                - name: beat-data
                  mountPath: /usr/share/filebeat/data
          volumes:
            - name: data
              emptydir: {}
            - name: beat-data
              emptydir: {}
eck-logstash:
  enabled: true
  # This is required to be able to set the logstash
  # output of beats in a consistent manner.
  fullnameOverride: "logstash-ls-beats"
  elasticsearchRefs:
    # This clusterName is required to match the environment variables
    # used in the below config.string output section.
    - clusterName: eck
      name: elasticsearch
  volumeClaimTemplates:
  - metadata:
      name: elasticsearch-data # Do not change this name unless you set up a corresponding persistentVolumeClaim to fulfill the request.
    spec:
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 100Gi
      storageClassName: managed-csi
  config:
    #nothing for now
  podTemplate:
    spec:
      containers:
      - name: logstash
        resources:
          requests:
            memory: 4Gi
            cpu: 4
          limits:
            memory: 8Gi
            cpu: 8
        #env:
        #- name: LS_JAVA_OPTS
        #  value: -Xms4096m -Xmx8192m
  pipelines:
    - pipeline.id: http
      config.string: |
        # HTTP/S event collector
        input {
          http {
            id => "http"
            port => "7080"
            type => hec
            ssl_enabled => false
          }
        }  
        output {
          elasticsearch {
            index => "logstash-hec-%{+YYYY.MM.dd}"
            hosts => [ "${ECK_ES_HOSTS}" ]
            user => "${ECK_ES_USER}"
            password => "${ECK_ES_PASSWORD}"
            ssl_verification_mode => none
            #ssl_certificate_authorities => "${ECK_ES_SSL_CERTIFICATE_AUTHORITY}"
          }
        }
    - pipeline.id: syslog
      config.string: |
        # TCP and UDP Syslog inputs
        input {
          tcp {
            port => 8514
            type => waf-logs
            codec => plain { charset=> "ISO-8859-1" }
          }          
          syslog {
            port => 7514
            type => syslog
            codec => plain { charset=> "ASCII" }
          }
          udp {
            port => 7514
            type => syslog
            codec => plain { charset=> "ASCII" }
          }
        }
        filter {
          if [type] == "syslog" {
            grok {
              match => { "message" => "%{SYSLOGTIMESTAMP:syslog_timestamp} %{SYSLOGHOST:syslog_hostname} %{DATA:syslog_program}(?:\[%{POSINT:syslog_pid}\])?: %{GREEDYDATA:syslog_message}" }
              break_on_match => false
              add_field => [ "received_at", "%{@timestamp}" ]
              add_field => [ "received_from", "%{host}" ]
            }
            mutate {
              replace => { "[@metadata][index_prefix]" => "syslog" }
            }
          }
          if [type] == "waf-logs" {
              grok {
                match => {
                  "message" => [
                  ",attack_type=\"%{DATA:attack_type}\"",
                  ",blocking_exception_reason=\"%{DATA:blocking_exception_reason}\"",
                  ",bot_anomalies=\"%{DATA:bot_anomalies}\"",
                  ",bot_category=\"%{DATA:bot_category}\"",
                  ",bot_signature_name=\"%{DATA:bot_signature_name}\"",
                  ",client_application=\"%{DATA:client_application}\"",
                  ",client_application_version=\"%{DATA:client_application_version}\"",
                  ",client_class=\"%{DATA:client_class}\"",
                  ",date_time=\"%{DATA:date_time}\"",
                  ",dest_port=\"%{DATA:dest_port}\"",
                  ",enforced_bot_anomalies=\"%{DATA:enforced_bot_anomalies}\"",
                  ",grpc_method=\"%{DATA:grpc_method}\"",
                  ",grpc_service=\"%{DATA:grpc_service}\"",
                  ",ip_client=\"%{DATA:ip_client}\"",
                  ",is_truncated=\"%{DATA:is_truncated}\"",
                  ",method=\"%{DATA:method}\"",
                  ",outcome=\"%{DATA:outcome}\"",
                  ",outcome_reason=\"%{DATA:outcome_reason}\"",
                  ",policy_name=\"%{DATA:policy_name}\"",
                  ",protocol=\"%{DATA:protocol}\"",
                  ",request_status=\"%{DATA:request_status}\"",
                  ",request=\"%{DATA:request}\"",
                  ",request_body_base64=\"%{DATA:request_body_base64}\"",
                  ",response_code=\"%{DATA:response_code}\"",
                  ",severity=\"%{DATA:severity}\"",
                  ",sig_cves=\"%{DATA:sig_cves}\"",
                  ",sig_ids=\"%{DATA:sig_ids}\"",
                  ",sig_names=\"%{DATA:sig_names}\"",
                  ",sig_set_names=\"%{DATA:sig_set_names}\"",
                  ",src_port=\"%{DATA:src_port}\"",
                  ",staged_sig_cves=\"%{DATA:staged_sig_cves}\"",
                  ",staged_sig_ids=\"%{DATA:staged_sig_ids}\"",
                  ",staged_sig_names=\"%{DATA:staged_sig_names}\"",
                  ",staged_threat_campaign_names=\"%{DATA:staged_threat_campaign_names}\"",
                  ",sub_violations=\"%{DATA:sub_violations}\"",
                  ",support_id=\"%{DATA:support_id}\"",
                  ",threat_campaign_names=\"%{DATA:threat_campaign_names}\"",
                  ",unit_hostname=\"%{DATA:unit_hostname}\"",
                  ",uri=\"%{DATA:uri}\"",
                  ",violations=\"%{DATA:violations}\"",
                  ",violation_details=\"%{DATA:violation_details_xml}\"",
                  ",violation_rating=\"%{DATA:violation_rating}\"",
                  ",vs_name=\"%{DATA:vs_name}\"",
                  ",x_forwarded_for_header_value=\"%{DATA:x_forwarded_for_header_value}\""
                ]
              }
              break_on_match => false
            }
            if [violation_details_xml] != "N/A" {
              xml {
                source => "violation_details_xml"
                target => "violation_details"
              }
            }
            mutate {
              split => { "attack_type" => "," }
              split => { "sig_cves" => "," }
              split => { "sig_ids" => "," }
              split => { "sig_names" => "," }
              split => { "sig_set_names" => "," }
              split => { "staged_sig_cves" => "," }
              split => { "staged_sig_ids" => "," }
              split => { "staged_sig_names" => "," }
              split => { "staged_threat_campaign_names" => "," }
              split => { "sub_violations" => "," }
              split => { "threat_campaign_names" => "," }
              split => { "violations" => "," }
              replace => { "[@metadata][index_prefix]" => "waf-logs" }
              remove_field => [
                "[violation_details][violation_masks]",
                "violation_details_xml",
                "message"
              ]
            }
          }
        }
        output {
          stdout { codec => plain }
          elasticsearch {
            index => "logstash-%{[@metadata][index_prefix]}-%{+YYYY.MM.dd}"
            hosts => [ "${ECK_ES_HOSTS}" ]
            user => "${ECK_ES_USER}"
            password => "${ECK_ES_PASSWORD}"
            ssl_verification_mode => none
            #ssl_certificate_authorities => "${ECK_ES_SSL_CERTIFICATE_AUTHORITY}"
          }
        }
    - pipeline.id: beats
      config.string: |
        #filebeat input
        input {
          beats {
            port => 5044
          }
        }
        filter {
          grok {
            match => { "message" => "%{HTTPD_COMMONLOG}"}
          }
          geoip {
            source => "[source][address]"
            target => "[source]"
          }
        }
        output {
          elasticsearch {
            hosts => [ "${ECK_ES_HOSTS}" ]
            user => "${ECK_ES_USER}"
            password => "${ECK_ES_PASSWORD}"
            ssl_verification_mode => none
            #ssl_certificate_authorities => "${ECK_ES_SSL_CERTIFICATE_AUTHORITY}"
          }
        }
  services:
    - name: beats
      service:
        spec:
          type: ClusterIP
          ports:
            - port: 5044
              name: "filebeat"
              protocol: TCP
              targetPort: 5044
    - name: syslog-f5-tcp
      service:
        spec:
          type: ClusterIP
          ports:
            - port: 8514
              name: "syslog-f5-tcp"
              protocol: TCP
              targetPort: 8514
    - name: syslog-tcp
      service:
        spec:
          type: ClusterIP
          ports:
            - port: 7514
              name: "syslog-tcp"
              protocol: TCP
              targetPort: 7514
    - name: syslog-udp
      service:
        spec:
          type: ClusterIP
          ports:
            - port: 7514
              name: "syslog-udp"
              protocol: UDP
              targetPort: 7514
    - name: hec
      service:
        spec:
          type: ClusterIP
          ports:
            - port: 7080
              name: "http-event-collector"
              protocol: TCP
              targetPort: 7080